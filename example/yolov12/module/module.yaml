Conv:
  # in_ch, out_ch, kernel, stride, padding, dilation, groups, act
  args: [c1, c2, k: 1, s: 1, p: None, g: 1, d: 1, act: nn.SiLU]
  exec: |
    def auto_pad(k, p=None, d=1):
        k = d * (k - 1) + 1 if isinstance(k, int) else [d * (x - 1) + 1 for x in k]
        return p or k // 2 if isinstance(k, int) else [x // 2 for x in k]
  vars:
    - p: auto_pad(k, None, d)
  layers:
    - [-1, nn.Conv2d, [c1, c2, k, s, p, d, g], { bias: False }]
    - [-1, nn.BatchNorm2d, [c2]]
    - [-1, act]

Bottleneck:
  args: [c1, c2, shortcut: True, g: 1, k: "(3, 3)", e: 0.5]
  vars:
    - hid_c: int(c2 * e)
    - need_add: shortcut and c1 == c2
    - out_from: "[0, -1] if need_add else -1"
    - add: "lambda x, y: x + y"
  layers:
    - [-1, Conv, [c1, hid_c, "k[0]", 1]]
    - [-1, Conv, [hid_c, c2, "k[1]", 1], { g: g }]
    - [out_from, add if need_add else nn.Identity]

C3k:
  args: [c1, c2, n: 1, shortcut: True, g: 1, e: 0.5, k: 3]
  vars:
    - hid_c: int(c2 * e)
    - bottleneck_args: "[hid_c, hid_c, shortcut, g, (k, k), 1]"
  layers:
    - [-1, Conv, [c1, hid_c, 1, 1]] # cv1
    - "[[-1, Bottleneck, bottleneck_args] for _ in range(n)]" # bottleneck
    - [0, Conv, [c1, hid_c, 1, 1]] # cv2
    - [[-2, -1], "lambda *x: torch.cat(x, 1)"] # cat
    - [-1, Conv, [2 * hid_c, c2, 1]] # cv3

C3k2:
  args: [c1, c2, n: 1, c3k: False, e: 0.5, g: 1, shortcut: True]
  vars:
    - hid_c: int(c2 * e)
    - bottleneck: C3k if c3k else Bottleneck
    - bottleneck_args: "[hid_c, hid_c, 2, shortcut, g] if c3k else [hid_c, hid_c, shortcut, g]"
    - bottleneck_former: "lambda i: {-1: 1} if i == 0 else -1"
    - cat_from: "[{-n-1: 0}, {-n-1: 1}] + list(range(-n, 0))"
  layers:
    - [-1, Conv, [c1, hid_c * 2, 1, 1]]
    - [-1, "lambda x: x.chunk(2, 1)"]
    - "[[bottleneck_former(i), bottleneck, bottleneck_args] for i in range(n)]"
    - [cat_from, "lambda *x: torch.cat(x, 1)"]
    - [-1, Conv, [hid_c * (2 + n), c2, 1]]

AAttn:
  args: [dim, num_heads, area: 1]
  exec: |
    self.area = area
    self.num_heads = num_heads
    self.head_dim = head_dim = dim // num_heads
    all_head_dim = head_dim * self.num_heads

    self.qkv = Conv(dim, all_head_dim * 3, 1, act=nn.Identity)
    self.pe = Conv(all_head_dim, dim, 7, 1, 3, g=dim, act=nn.Identity)
    self.proj = Conv(all_head_dim, dim, 1, act=nn.Identity)

    def forward(x):
        B, C, H, W = x.shape
        N = H * W
        qkv = self.qkv(x).flatten(2).transpose(1, 2)
        if self.area > 1:
            qkv = qkv.reshape(B * self.area, N // self.area, C * 3)
            B, N, _ = qkv.shape
        q, k, v = (
            qkv.view(B, N, self.num_heads, self.head_dim * 3)
            .permute(0, 2, 3, 1)
            .split([self.head_dim, self.head_dim, self.head_dim], dim=2)
        )
        attn = (q.transpose(-2, -1) @ k) * (self.head_dim**-0.5)
        attn = attn.softmax(dim=-1)
        x = v @ attn.transpose(-2, -1)
        x = x.permute(0, 3, 1, 2)
        v = v.permute(0, 3, 1, 2)
        if self.area > 1:
            x = x.reshape(B // self.area, N * self.area, C)
            v = v.reshape(B // self.area, N * self.area, C)
            B, N, _ = x.shape
        x = x.reshape(B, H, W, C).permute(0, 3, 1, 2).contiguous()
        v = v.reshape(B, H, W, C).permute(0, 3, 1, 2).contiguous()
        x = x + self.pe(v)
        return self.proj(x)
  layers:
    - [-1, forward]

ABlock:
  args: [dim, num_heads, mlp_ratio: 1.2, area: 1]
  vars:
    - mlp_hidden_dim: int(dim * mlp_ratio)
    - add: "lambda x, y: x + y"
  layers:
    - [-1, AAttn, [dim, num_heads, area]]
    - [[-2, -1], add]
    - [-1, Conv, [dim, mlp_hidden_dim, 1]]
    - [-1, Conv, [mlp_hidden_dim, dim, 1], { act: nn.Identity }]
    - [[-3, -1], add]

A2:
  args: [dim, mlp_ratio: 2, area: 1]
  layers:
    - [-1, ABlock, [dim, dim//32, mlp_ratio, area]]
    - [-1, ABlock, [dim, dim//32, mlp_ratio, area]]

A2C2f:
  # prettier-ignore
  args: [c1, c2, n: 1, a2: True, area: 1, residual: False, mlp_ratio: 2, e: 0.5, g: 1, shortcut: True]
  params:
    - gamma: nn.Parameter(0.01 * torch.ones(c2), requires_grad=True) if a2 and residual else None
  vars:
    - hid_c: int(c2 * e)
    - c3k_backbone: "[[-1, C3k, [hid_c, hid_c, 2, shortcut, g]]]"
    - a2_backbone: "[[-1, A2, [hid_c, mlp_ratio, area]]]"
    - backbone: "a2_backbone if a2 else c3k_backbone"
    - cat_from: "list(range(-n - 1, 0))"
    - scaling: "lambda x, y: x + gamma.view(-1, len(gamma), 1, 1) * y if gamma is not None else y"
  layers:
    - [-1, Conv, [c1, hid_c, 1, 1]]
    - "backbone * n"
    - ["cat_from", "lambda *x: torch.cat(x, 1)"]
    - [-1, Conv, [hid_c * (1 + n), c2, 1]]
    - [[0, -1], scaling]
